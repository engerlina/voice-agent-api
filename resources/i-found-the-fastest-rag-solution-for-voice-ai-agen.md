---
title: "I Found The FASTEST RAG Solution For Voice AI Agents!"
url: https://www.youtube.com/watch?v=i_QrWRAC9NM
date: 2026-01-30
wordcount: 5282
author: "Ahmed Mukhtar | AI Automations"
channel: "Ahmed Mukhtar | AI Automations"
type: "youtube-transcript"
tags: []
---

# I Found The FASTEST RAG Solution For Voice AI Agents!

## Transcript

When it comes to voice AI systems, every millisecond counts. And implementing a fast rack architecture could make the difference between a conversational realistic voice AI system and one that leaves your users awkwardly waiting in silence. In this video, I'm going to show you exactly which rag setup gives you the lowest latency and the best accuracy. I have built a complete testing framework, ran a number of queries across four different vector databases, and the results, well, let's just say that one of these options absolutely dominated, and it might not be the one that you'd expect. So, with that being said, let's dive straight in. Here's the thing about voice AR agents that most people don't understand. When a user asks the voice AI agent a question, we have about 500 to 800 milliseconds before they notice a delay. In that time, we need to basically convert their speech to text using the SCT model. We process it through our LLM model and then if a question is asked, we search through our knowledge base. This is the rag component of the system. And once we have the results, we then generate a response, convert it back to speech via TTS, and then say it back to the user. And often times, all of the other models in this pipeline have been optimized to work in a very fast manner with very low latency because we can stream the tokens and that reduces the latency. However, when it comes to searching our knowledge base, we have to call a third party tool and this usually takes a long time to call the tool, do the search. We need to also embed our question into a vector. So we have to convert it into a vector, do the search in the vector database and then return the responses. So this is usually where the bottleneck happens whenever we're asking questions to our voice agent. And this is why I built this test to basically find out what is the best model for latency and also which model actually gives us more accurate results. And this is one thing to keep in mind is that we don't just want to be very fast. We also want to make sure that we're getting the correct responses. Otherwise, it's better to be a little bit slower and more accurate than just to be very fast with the responses and give our customers the wrong information. So, it's a fine balance between the two. So, before we look at the demo and the results, let's quickly look at what I've built to test out the different vector databases. So, first I want you to understand how I set this up so you can see that it's a fair comparison because the key here is that we keep everything exactly the same. We only change the vector database and a foundation of this system is the document ingestion where we take a PDF of a clinic. So, it's a dental clinic of around 20 pages in a PDF file. We pass all of the information in the exact same way. And the chunking strategy has to be exactly the same because different chunk sizes will completely invalidate the comparison. So, we wanted to make sure we ingest the document in the exact same manner. We chunk it in exact same manner. And we want to also use the same embedding generation. So, I used the OpenAI text embedding small model for all of the vector stores. The way I managed to keep everything exactly the same. So the code when I run the code, we don't actually change anything. We just want to change a single environment variable in our setup. So I'm using live kit. So I'm able to do this live inside of the dashboard. We go down to our secrets and then we have the vector store here environment. All I need to do is update this secret and we just have to name it one of these local pine cone superbase or or QR and it will automatically use that vector store. So this way we can make sure that all of the tests are exactly the same across the different vector databases. So the four different setups that I'm using, I am testing a local setup inside of the lifekit agent where we have base. So it's an efficient similarity search library and it allows us to do the similarity searches which is basically what happens when we query our vector databases. So very powerful and very known library that I decided to use and we actually keep it local to the agent. So when we host the agent somewhere whenever we're doing rag, we're actually talking in the same instance as the AI agent is hosted in. So there's no external dependencies. Next, we also have Pineon, which is the most popular cloud-based vector database. It's also serverless and it's very scalable. So, inside of Pineon, this is what it looks like. We have 21 different records. These are the different chunks. Inside of the index, we're using the dimensions here. And we're using a cosine for the metric. And we can also use superbase as the vector database by using PG vector. And basically inside of here, this is how it looks like. So, we've got our embeddings table. Again we have 21 records because we've chunked the information exactly the same. And this is how it looks like inside of our superbase. And finally we have hugrant or quadrant depending on how you want to say this. And similarly we have our vector store index hosted inside of here. We have 21 points and the status is green. It's good to go. And finally I created a metric tracker. And this is where we want to basically capture the embedding latency uh the total latency. So from end to end rag timeline. We also want to measure the score. So the score be able to see how confident the model is, which is basically like the similarity score between the query and the returned results. And we also want to look at the average score. And finally, we just want to capture the number of results that were returned. This is actually also another environment variable that's set to three. So we only returning three different documents because the more documents that you return, the higher the latency. So I think three is a good balance between having enough context and also not having too much context that it delays the model. One thing I also want to cover is the how I classify the system and we have the different latency statuses. So for anything below 500 milliseconds, we say this is good. It's conversational for the voice AI. Medium is 500 to 1,000 milliseconds. So half a second to a second. And anything above that is basically just awkward silence. And this is how I've marked it in the dashboard which I'll show you guys later after the demo. The quality statuses are also the scores like I mentioned above here. So this is the similarity results between the query and the results. So, anything above 0.8 is excellent, which is quite high. Uh, usually with uh similarity searches, you're looking around the 0.7 range. Um, so I should probably reduce this slightly. Anything from 0.6 to 0.8 is good. And then below that is poor and probably irrelevant to our query. So now we have seen all of the different vector stores. Let's go ahead and look at the agent in action. Right. So to demonstrate the agent, I have the agent hosted inside of LifeKit and I'm using the LifeKit agent playground to quickly chat with the agent. So the agent's already hosted and I believe currently is set to the local uh database. So let's go ahead and connect. Uh hello. Hello. How can I assist you with your dental needs today? I wanted to ask you what is the cost for your composite filing for one surface? The cost for a composite filling for one surface at Bright Smile Dental Clinic is $185. Insurance copay estimates typically range from $40 to $75 depending on your coverage. Would you like information on scheduling an appointment? Uh, yes. I wanted to ask you which doctor uh speaks Arabic. Dr. Aisha Hassan, DDS, who practices general dentistry at the Mesa and Tempe locations, speaks Arabic. Would you like help scheduling an appointment with her? Goodbye. But so that was using the local setup. And if we want to change, like I mentioned, we have to just go down to our secrets here, go to the vector store, and change this to, for example, let's do use pine cone. Save changes. And then we can go and look at the logs of the agent just to make sure that it's actually using the correct vector database. So if I go ahead and connect, uh, I should be able to see, yeah, pinecoin has been initiated. Awesome. Let's wait for the agent to join. Hello. Hello. I am Sarah, your voice assistant for Bright Smile Dental Clinic. How can I assist you today? Hi Sarah. I wanted to know how long does your appointment for root canal take? For a mer. A root canal appointment for a mer at Bright Smile Dental Clinic typically takes about 90 minutes. If you need more information, thank you. So with that being said, just a very quick demo of seeing the agent in action. Uh you can see the latency is a little bit more when it comes to Pinecoin. However, let's look at the final results. So I've tested about 10 different questions for each of the vector databases to get an idea of the kind of latency and accuracy that we get with each one. So at the end of the call, we have all of the different questions sent to a NA10 web hook, which basically processes the metrics and updates a Google sheet with that data. And then finally I built this dashboard to basically connect with that data sheet and gives us a overview of the different results. So as we can see here we have 40 different queries across all of the different uh vector stores that we have. Instantly you're able to see that superbase takes a very long time here and this is due to the search latency. So embedding latency is when we embed the query to convert it into a vector and then we have to do a search. So we have two different latencies here that make up the total latency for the search. So we can see here that QN was at the 800 to 900 millisecond range for the total latency. Pine cone was a little bit better at only 450 which is actually really good. So Pineon is actually a very good fast vector store. And then we have Superbase which is super high and that's due to the search latency. So the embedding only took around 300 millconds. However, the search latency takes way too long, almost 1.7 seconds to do the searching. This is because Superbase is not actually designed to be used as a very fast vector database. And finally, we have our local vector store, which was around 340 or 350 milliseconds. And you can see that most of the latency was actually due to the embedding part of the query. And you can see all of these are kind of the same uh because they all use OpenAI's embedding. The search was actually super super fast at 0.3 milliseconds. So this is why local posting of the rag is actually really good. If you go over and look at the quality of the scores. So the average score of the results returned. Quang gave us an average score of 0.6 with the highest result being 0.63. And then for pine con was around the same. We have 0.58 with the highest score of 0.6. And superface again was very similar. And then local phase was also very similar but the top results you can see here was the highest. So with the local vector store we actually had slightly higher results. So if we look at the kind of performance quality distribution here you can see superbase is right at the the other end of the spectrum and pine cone and the local setup kind of average here around the left hand side and then you have quant in the middle and then at the bottom here we just see some of the recent queries that we asked the agent the latency and the quality score. So and if we want to test some more data all we need to do is talk with the agent and then click refresh and this will automatically pull in all of the new questions and queries from our Google sheets. Right. So based on my testing, I have some practical recommendations for you guys. So for voice AI applications, we want to use the local setup. And this is if we have a relatively small knowledge base around 100,000 vectors, which should cover most of the business use cases. This is because if we increase the size, the latency will increase. And also, I'm not quite sure if we're able to even upload such large knowledge base inside of the life kit. Unless you're self-hosting. If you're self-hosting, you can probably increase this. And if you have very good fast GPUs, you can probably increase the knowledge base. However, this is just a good ballpark figure for you guys to know about. And I say this because based on my test, the average latency was super low. 90% of the queries were actually fast and there was no slow queries. Also, best quality scores for testing. There are zero infrastructure cost and there's no network dependencies and external app dependencies. So, this is why I highly recommend using the local phase library. I'd consider Pinecoin if you have a bit more of a budget. So it cost a little bit more. Uh you need team collaboration, you need serverless setup that is very fast, has very low latency and is scalable. You can also consider Q jump when you want to have a open- source solution similar to Pinecon with the monitoring and everything. And we want to avoid Superbase because the latency was ridiculous. So I'd actually only use superbase PG vector for chat kind of interfaces where the users kind of expect the weight and it's great if you want to have a unified database and plus the vector database all in one place. With that being said, there are many ways to also improve the accuracy of the results when you're using rag systems and also improve the latency. So there are many parameters that we can play around with such as the top K, the chunking size, caching frequent queries and etc. So, if you want me to go into a bit more depth into how we go about improving our rag system, let me know in the comments down below. This was actually quite a fun and valuable project for myself as I've been building a lot of voice AI solutions for clients. And I hope that you guys have also found some value in this. Make sure you hit that like and subscribe button as always. And you want me to do a more deep dive into any of the stuff that we've covered, make sure you let me know in the comments down below. Thank you, and I'll see you in the next one. Peace. When it comes to voice AI systems, every millisecond counts. And implementing a fast rack architecture could make the difference between a conversational realistic voice AI system and one that leaves your users awkwardly waiting in silence. In this video, I'm going to show you exactly which rag setup gives you the lowest latency and the best accuracy. I have built a complete testing framework, ran a number of queries across four different vector databases, and the results, well, let's just say that one of these options absolutely dominated, and it might not be the one that you'd expect. So, with that being said, let's dive straight in. Here's the thing about voice AR agents that most people don't understand. When a user asks the voice AI agent a question, we have about 500 to 800 milliseconds before they notice a delay. In that time, we need to basically convert their speech to text using the SCT model. We process it through our LLM model and then if a question is asked, we search through our knowledge base. This is the rag component of the system. And once we have the results, we then generate a response, convert it back to speech via TTS, and then say it back to the user. And often times, all of the other models in this pipeline have been optimized to work in a very fast manner with very low latency because we can stream the tokens and that reduces the latency. However, when it comes to searching our knowledge base, we have to call a third party tool and this usually takes a long time to call the tool, do the search. We need to also embed our question into a vector. So we have to convert it into a vector, do the search in the vector database and then return the responses. So this is usually where the bottleneck happens whenever we're asking questions to our voice agent. And this is why I built this test to basically find out what is the best model for latency and also which model actually gives us more accurate results. And this is one thing to keep in mind is that we don't just want to be very fast. We also want to make sure that we're getting the correct responses. Otherwise, it's better to be a little bit slower and more accurate than just to be very fast with the responses and give our customers the wrong information. So, it's a fine balance between the two. So, before we look at the demo and the results, let's quickly look at what I've built to test out the different vector databases. So, first I want you to understand how I set this up so you can see that it's a fair comparison because the key here is that we keep everything exactly the same. We only change the vector database and a foundation of this system is the document ingestion where we take a PDF of a clinic. So, it's a dental clinic of around 20 pages in a PDF file. We pass all of the information in the exact same way. And the chunking strategy has to be exactly the same because different chunk sizes will completely invalidate the comparison. So, we wanted to make sure we ingest the document in the exact same manner. We chunk it in exact same manner. And we want to also use the same embedding generation. So, I used the OpenAI text embedding small model for all of the vector stores. The way I managed to keep everything exactly the same. So the code when I run the code, we don't actually change anything. We just want to change a single environment variable in our setup. So I'm using live kit. So I'm able to do this live inside of the dashboard. We go down to our secrets and then we have the vector store here environment. All I need to do is update this secret and we just have to name it one of these local pine cone superbase or or QR and it will automatically use that vector store. So this way we can make sure that all of the tests are exactly the same across the different vector databases. So the four different setups that I'm using, I am testing a local setup inside of the lifekit agent where we have base. So it's an efficient similarity search library and it allows us to do the similarity searches which is basically what happens when we query our vector databases. So very powerful and very known library that I decided to use and we actually keep it local to the agent. So when we host the agent somewhere whenever we're doing rag, we're actually talking in the same instance as the AI agent is hosted in. So there's no external dependencies. Next, we also have Pineon, which is the most popular cloud-based vector database. It's also serverless and it's very scalable. So, inside of Pineon, this is what it looks like. We have 21 different records. These are the different chunks. Inside of the index, we're using the dimensions here. And we're using a cosine for the metric. And we can also use superbase as the vector database by using PG vector. And basically inside of here, this is how it looks like. So, we've got our embeddings table. Again we have 21 records because we've chunked the information exactly the same. And this is how it looks like inside of our superbase. And finally we have hugrant or quadrant depending on how you want to say this. And similarly we have our vector store index hosted inside of here. We have 21 points and the status is green. It's good to go. And finally I created a metric tracker. And this is where we want to basically capture the embedding latency uh the total latency. So from end to end rag timeline. We also want to measure the score. So the score be able to see how confident the model is, which is basically like the similarity score between the query and the returned results. And we also want to look at the average score. And finally, we just want to capture the number of results that were returned. This is actually also another environment variable that's set to three. So we only returning three different documents because the more documents that you return, the higher the latency. So I think three is a good balance between having enough context and also not having too much context that it delays the model. One thing I also want to cover is the how I classify the system and we have the different latency statuses. So for anything below 500 milliseconds, we say this is good. It's conversational for the voice AI. Medium is 500 to 1,000 milliseconds. So half a second to a second. And anything above that is basically just awkward silence. And this is how I've marked it in the dashboard which I'll show you guys later after the demo. The quality statuses are also the scores like I mentioned above here. So this is the similarity results between the query and the results. So, anything above 0.8 is excellent, which is quite high. Uh, usually with uh similarity searches, you're looking around the 0.7 range. Um, so I should probably reduce this slightly. Anything from 0.6 to 0.8 is good. And then below that is poor and probably irrelevant to our query. So now we have seen all of the different vector stores. Let's go ahead and look at the agent in action. Right. So to demonstrate the agent, I have the agent hosted inside of LifeKit and I'm using the LifeKit agent playground to quickly chat with the agent. So the agent's already hosted and I believe currently is set to the local uh database. So let's go ahead and connect. Uh hello. Hello. How can I assist you with your dental needs today? I wanted to ask you what is the cost for your composite filing for one surface? The cost for a composite filling for one surface at Bright Smile Dental Clinic is $185. Insurance copay estimates typically range from $40 to $75 depending on your coverage. Would you like information on scheduling an appointment? Uh, yes. I wanted to ask you which doctor uh speaks Arabic. Dr. Aisha Hassan, DDS, who practices general dentistry at the Mesa and Tempe locations, speaks Arabic. Would you like help scheduling an appointment with her? Goodbye. But so that was using the local setup. And if we want to change, like I mentioned, we have to just go down to our secrets here, go to the vector store, and change this to, for example, let's do use pine cone. Save changes. And then we can go and look at the logs of the agent just to make sure that it's actually using the correct vector database. So if I go ahead and connect, uh, I should be able to see, yeah, pinecoin has been initiated. Awesome. Let's wait for the agent to join. Hello. Hello. I am Sarah, your voice assistant for Bright Smile Dental Clinic. How can I assist you today? Hi Sarah. I wanted to know how long does your appointment for root canal take? For a mer. A root canal appointment for a mer at Bright Smile Dental Clinic typically takes about 90 minutes. If you need more information, thank you. So with that being said, just a very quick demo of seeing the agent in action. Uh you can see the latency is a little bit more when it comes to Pinecoin. However, let's look at the final results. So I've tested about 10 different questions for each of the vector databases to get an idea of the kind of latency and accuracy that we get with each one. So at the end of the call, we have all of the different questions sent to a NA10 web hook, which basically processes the metrics and updates a Google sheet with that data. And then finally I built this dashboard to basically connect with that data sheet and gives us a overview of the different results. So as we can see here we have 40 different queries across all of the different uh vector stores that we have. Instantly you're able to see that superbase takes a very long time here and this is due to the search latency. So embedding latency is when we embed the query to convert it into a vector and then we have to do a search. So we have two different latencies here that make up the total latency for the search. So we can see here that QN was at the 800 to 900 millisecond range for the total latency. Pine cone was a little bit better at only 450 which is actually really good. So Pineon is actually a very good fast vector store. And then we have Superbase which is super high and that's due to the search latency. So the embedding only took around 300 millconds. However, the search latency takes way too long, almost 1.7 seconds to do the searching. This is because Superbase is not actually designed to be used as a very fast vector database. And finally, we have our local vector store, which was around 340 or 350 milliseconds. And you can see that most of the latency was actually due to the embedding part of the query. And you can see all of these are kind of the same uh because they all use OpenAI's embedding. The search was actually super super fast at 0.3 milliseconds. So this is why local posting of the rag is actually really good. If you go over and look at the quality of the scores. So the average score of the results returned. Quang gave us an average score of 0.6 with the highest result being 0.63. And then for pine con was around the same. We have 0.58 with the highest score of 0.6. And superface again was very similar. And then local phase was also very similar but the top results you can see here was the highest. So with the local vector store we actually had slightly higher results. So if we look at the kind of performance quality distribution here you can see superbase is right at the the other end of the spectrum and pine cone and the local setup kind of average here around the left hand side and then you have quant in the middle and then at the bottom here we just see some of the recent queries that we asked the agent the latency and the quality score. So and if we want to test some more data all we need to do is talk with the agent and then click refresh and this will automatically pull in all of the new questions and queries from our Google sheets. Right. So based on my testing, I have some practical recommendations for you guys. So for voice AI applications, we want to use the local setup. And this is if we have a relatively small knowledge base around 100,000 vectors, which should cover most of the business use cases. This is because if we increase the size, the latency will increase. And also, I'm not quite sure if we're able to even upload such large knowledge base inside of the life kit. Unless you're self-hosting. If you're self-hosting, you can probably increase this. And if you have very good fast GPUs, you can probably increase the knowledge base. However, this is just a good ballpark figure for you guys to know about. And I say this because based on my test, the average latency was super low. 90% of the queries were actually fast and there was no slow queries. Also, best quality scores for testing. There are zero infrastructure cost and there's no network dependencies and external app dependencies. So, this is why I highly recommend using the local phase library. I'd consider Pinecoin if you have a bit more of a budget. So it cost a little bit more. Uh you need team collaboration, you need serverless setup that is very fast, has very low latency and is scalable. You can also consider Q jump when you want to have a open- source solution similar to Pinecon with the monitoring and everything. And we want to avoid Superbase because the latency was ridiculous. So I'd actually only use superbase PG vector for chat kind of interfaces where the users kind of expect the weight and it's great if you want to have a unified database and plus the vector database all in one place. With that being said, there are many ways to also improve the accuracy of the results when you're using rag systems and also improve the latency. So there are many parameters that we can play around with such as the top K, the chunking size, caching frequent queries and etc. So, if you want me to go into a bit more depth into how we go about improving our rag system, let me know in the comments down below. This was actually quite a fun and valuable project for myself as I've been building a lot of voice AI solutions for clients. And I hope that you guys have also found some value in this. Make sure you hit that like and subscribe button as always. And you want me to do a more deep dive into any of the stuff that we've covered, make sure you let me know in the comments down below. Thank you, and I'll see you in the next one. Peace.